{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf35a41-8183-46e2-8fd0-ee0ad29bb423",
   "metadata": {},
   "source": [
    "# Clandestine Runway Detection\n",
    "This notebook presents a deep learning approach to detect clandestine or unauthorized runways in satellite imagery. Clandestine runways pose security risks, as they can be used for illegal activities such as unauthorized landings, smuggling, and other covert operations. The goal of this project is to identify and localize active clandestine runways in remote regions using high-resolution satellite data, making it easier for authorities and organizations to monitor and address these hidden threats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216b822-6b83-4b3a-a7ac-9ee389d35165",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c257e7-e1b3-49eb-863c-799f38870412",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e3beb6-1b6d-4077-a8dc-5aaaf2bf825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7fbfea-0179-44c4-b6f6-21bc2d59b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import toml\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimage.transform import resize\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import TFSegformerModel, TFSegformerDecodeHead, SegformerConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utils import *\n",
    "from src.model import DualSegformerClassifierModel, ClearMemoryCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4385c4-9f21-4ac2-b76c-a5068780e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpu device available\n",
    "assert len(tf.config.experimental.list_physical_devices('GPU')) > 0, 'This notebook requires GPU to run!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73b54b-b975-4f68-a78f-18a66293e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set notebook memory limit\n",
    "tf.config.LogicalDeviceConfiguration(memory_limit=7500)\n",
    "torch.cuda.set_per_process_memory_fraction(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d8ba0c-caf3-4c66-8564-500420c22dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to: 1234\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f139b1-b0ab-46f4-a24c-c146ba4e4fe2",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52b9857-9ed9-4596-be38-db80b3804cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'params', 'output', 'patches'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = toml.load('./config.toml')\n",
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5009c1-ee1b-4074-b619-cf250015c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = config['data']['image_data']['train_image_dir']\n",
    "test_dir = config['data']['image_data']['test_image_dir']\n",
    "logs_dir = config['output']['logs_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e97a76-8d4a-4e17-b3da-25615970a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = config['params']['model']['batch_size']\n",
    "HEIGHT = config['params']['data']['height']\n",
    "WIDTH = config['params']['data']['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b2551-1483-4ffa-88fc-3ee0d6961750",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea4357-2ec7-4ffb-a1a3-253d8b4bdb62",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ef6592-ee5e-4e98-9088-63eebf7bc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    '''Loads data pairs of tif image and corresponding mask\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to folder containing raster-mask pairs\n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarrays\n",
    "    '''\n",
    "    # Fetch files\n",
    "    images = [os.path.join(path, i) for i in os.listdir(path)]\n",
    "    image_feature = [i for i in images if 'sdata' in i][0]\n",
    "    image_target = [i for i in images if 'target' in i][0]\n",
    "\n",
    "    # Read target mask\n",
    "    with rio.open(image_target) as src:\n",
    "        target_bounds = src.bounds\n",
    "        arr_target = src.read( \n",
    "            out_shape = (256, 256), # output size hardcoded to 256 8 256\n",
    "            resampling = Resampling.bilinear\n",
    "        )\n",
    "        arr_target = np.moveaxis(arr_target, 0, -1)\n",
    "        \n",
    "        # some non examples got corrupted during saving\n",
    "        # pending new download fix..\n",
    "        # the condition below manually fixes the problem\n",
    "        if path[-1] == '0' and arr_target.sum() > 0:\n",
    "            arr_target *= 0\n",
    "        src.close()\n",
    "\n",
    "    # Read image mask\n",
    "    with rio.open(image_feature) as src2:\n",
    "        arr_feature = src2.read(\n",
    "            out_shape = (src2.count, 512, 512), # size hardcoded to segformer 512_512\n",
    "            resampling = Resampling.nearest\n",
    "        )\n",
    "        arr_feature = np.moveaxis(arr_feature, 0, -1)\n",
    "        src2.close()\n",
    "\n",
    "    # delete vars to save memeory\n",
    "    del images, image_feature, image_target\n",
    "    \n",
    "    return (arr_feature, arr_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd92e11-e98d-4675-b166-fa1819b2c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(path, num_samples, cmap = 'Spectral_r', random_state = 1):\n",
    "    '''\n",
    "    Helper function to visualize samples\n",
    "    Parameters\n",
    "    path : str\n",
    "        Path to folder containing image mask pairs.\n",
    "    num_samples : int\n",
    "        Number of samples to plot. Maximum of six (6).\n",
    "    cmap : str\n",
    "        Matplotlib cmap for visualization.\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    # clip number of samples\n",
    "    num_samples = min(num_samples, 6)\n",
    "\n",
    "    # set random state\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # select samples\n",
    "    samples = np.random.choice(path, num_samples, replace = False)\n",
    "        \n",
    "    # set figure\n",
    "    fig, ax = plt.subplots(2, num_samples, figsize = (24, 6))\n",
    "\n",
    "    # plot images\n",
    "    for i in range(num_samples):\n",
    "        img, targ = load_images(samples[i])\n",
    "        img = np.clip(img[:, :, [2, 1, 0]] / 1000, 0, 1)       \n",
    "        ax[0][i].imshow(img, cmap = cmap)\n",
    "        ax[1][i].imshow(targ, cmap = 'gray')\n",
    "        ax[0][i].axis('off')\n",
    "        ax[1][i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d0a58-8f4c-49a0-a99f-1719621818b7",
   "metadata": {},
   "source": [
    "## Load and view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e15acbf4-25a0-47fb-94ff-124464818857",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load data from train download log\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_download_log \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(logs_dir, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(logs_dir) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_params.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m i][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m train_shp \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_dir, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(train_dir) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeojson\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m i][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m train_shp\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs'"
     ]
    }
   ],
   "source": [
    "# load data from train download log\n",
    "train_download_log = os.path.join(logs_dir, [i for i in os.listdir(logs_dir) if 'train_params.csv' in i][0])\n",
    "train_shp = [os.path.join(train_dir, i) for i in os.listdir(train_dir) if 'geojson' in i][0]\n",
    "train_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fc7ca-3951-4a06-a679-9d6d9de3ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read competition train data\n",
    "train_download_df = pd.read_csv(train_download_log)\n",
    "train_gdf = gpd.read_file(train_shp)\n",
    "\n",
    "# join to download log files\n",
    "train_gdf = train_gdf.set_index('filename').join(train_download_df.set_index('name'))\n",
    "\n",
    "# manually correct location with activo = 2 not caught in during download and too late to change at this point. \n",
    "train_gdf.loc[(train_gdf.Activo == 2) & (train_gdf.runway == 1), 'Activo'] = 1\n",
    "\n",
    "# reformat active\n",
    "# 0 = inactive, 1 = active, 2 = no runway\n",
    "train_gdf['Activo'] = abs(2 - train_gdf.Activo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013fa0-4db2-49ed-a0ef-71d82e055286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the error is fixed\n",
    "m = train_gdf[train_gdf.Activo == 0]\n",
    "m[m.id.duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a33cba-1c79-4cdc-932e-d77be61befa2",
   "metadata": {},
   "source": [
    "The download script could not retrieve suitable images from 2015. Filter out these points from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af56b0-8d9e-42f4-853a-679746ba2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select valid tbrain data\n",
    "valid_train_df = train_gdf[train_gdf.status == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb9032-d68c-4b5d-a49e-975c3fe0f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select train file directories from the dataframe\n",
    "train_files = valid_train_df.apply(lambda x: x.output_dir, axis = 1).to_list()\n",
    "train_target = valid_train_df['runway'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612f14d-7e00-4d9f-84b2-3db93cd95434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "plot_sample_images(train_files, 12, random_state = 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
